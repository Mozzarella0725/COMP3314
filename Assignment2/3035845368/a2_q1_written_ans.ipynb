{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AUwbFEaMiWi"
   },
   "source": [
    "# COMP3314 Assignment2-Q1: Written Question (40 Points)\n",
    "\n",
    "## Single Answer Questions (20 points, 2 points each) \n",
    "\n",
    "No need to implement any code. Please fill your answers in the following table. \n",
    "\n",
    "|   Q1   |  Q2   |   Q3   |  Q4   |   Q5  |   Q6  |   Q7  |   Q8  |   Q9  |  Q10  |  \n",
    "|  ----  | ----  |  ----  | ----  | ----  | ----  | ----  | ----  | ----  | ----  | \n",
    "|    C   |   A   |   C    |  A    |   B   |   B   |   C   |  D   |   A  |   D   |  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1\n",
    "\n",
    "Which one of the following statements about SVM is correct?\n",
    "\n",
    "A. Hinge loss is only applicable in hard margin SVM, and not in soft margin SVM.\n",
    "\n",
    "B. The maximum margin decision boundary generated by an SVM is always a straight line (hyperplane) in the original feature space.\n",
    "\n",
    "C. The \"kernel trick\" in SVM is primarily used to transform the feature space to make the data linearly separable, thereby improving the model's ability to capture complex patterns.\n",
    "\n",
    "D. The maximum margin decision boundary that an SVM generates has the lowest error among all linear classifiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2\n",
    "\n",
    "Which one of the following statements about SVM is correct?\n",
    "\n",
    "A. The regularization parameter $C$ in a soft margin SVM controls the trade-off between maximizing the margin and minimizing the classification error.\n",
    "\n",
    "B. SVM is inherently multi-class classifiers and do not require any additional strategies to handle multi-class problems.\n",
    "\n",
    "C. The \"kernel trick\" in SVM increases the computational complexity of the model exponentially with the number of features.\n",
    "\n",
    "D. SVM is most effective when the data is linearly separable and do not perform well with non-linear data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3\n",
    "\n",
    "Which of the following best describes the role of the regularization parameter $C$ in a soft-margin SVM?\n",
    "\n",
    "A. $C$ controls the trade-off between margin width and misclassification penalty, with higher $C$ values allowing more misclassifications.\n",
    "\n",
    "B. $C$ controls the width of the margin, with larger values of $C$ leading to wider margins.\n",
    "\n",
    "C. $C$ controls the trade-off between margin width and misclassification penalty, with higher $C$ values enforcing stricter classification and narrower margins.\n",
    "\n",
    "D. $C$ only affects the decision boundary for linearly separable data and has no impact on non-linear classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4\n",
    "\n",
    "Which one of the following statements about dimensionality reduction is correct?\n",
    "\n",
    "A. The projection relationship of PCA can always be represented by a 2D matrix.\n",
    "\n",
    "B. PCA gives the best low-rank approximation of a matrix by minimizing the absolute errors.\n",
    "\n",
    "C. PCA operation is reversible without information loss.\n",
    "\n",
    "D. PCA is a non-deterministic algorithm. If you apply PCA to the same training data, you may get slightly different results each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5\n",
    "\n",
    "Which of the following statements correctly describes the principal components in PCA?\n",
    "\n",
    "A. Principal components are the features with the highest variance and correspond to the original features with the largest coefficients.\n",
    "\n",
    "B. Principal components are the eigenvectors of the dataâ€™s covariance matrix, representing the directions of maximum variance in the data.\n",
    "\n",
    "C. Principal components are linear combinations of the data points that maximize the covariance between different features.\n",
    "\n",
    "D. Principal components are computed by selecting the features with the smallest variance to reduce noise in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6\n",
    "\n",
    "You apply PCA to a dataset with 150 features and find that the first 12 principal components explain 85% of the total variance. If the total variance of the dataset is calculated to be 1200, what is the minimum variance that must be explained by the first 12 principal components?\n",
    "\n",
    "A. 900\n",
    "\n",
    "B. 1020\n",
    "\n",
    "C. 1200\n",
    "\n",
    "D. 1500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7\n",
    "Which one of the following statements about KNN is correct?\n",
    "\n",
    "A. K denotes the number of classes.\n",
    "\n",
    "B. KNN is an unsupervised clustering method.\n",
    "\n",
    "C. The memory requirement of the KNN algorithm increases with the growth of the training data size.\n",
    "\n",
    "D. Increasing K improves model accuracy however makes the inference slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8\n",
    "\n",
    "Which one of the following statements about decision trees is correct?\n",
    "\n",
    "A. Decision trees can only be used for classification tasks, not regression tasks.\n",
    "\n",
    "B. Decision trees are invariant to the scaling of the input features, so normalization is unnecessary.\n",
    "\n",
    "C. The primary criterion for splitting nodes in a decision tree is the correlation coefficient of the features.\n",
    "\n",
    "D. Pruning a decision tree after it has been fully grown can help reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9\n",
    "Which one of the following statements about decision tree is correct?\n",
    "\n",
    "A. If a decision tree is overfitted to the training data, decreasing the max-depth usually is a good solution.\n",
    "\n",
    "B. If a decision tree is underfitted to the training data, normalizing the input data would be a good solution.\n",
    "\n",
    "C. The Gini impurity of a node is always lower than its parent node.\n",
    "\n",
    "D. Mean absolute error is commonly used to measure the impurity of a node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10\n",
    "\n",
    "In which of the following scenarios would you be more likely to use standardization over normalization?\n",
    "\n",
    "A. When preparing image pixel values for a machine learning model to ensure they are within the range [0, 1].\n",
    "\n",
    "B. When preprocessing data for a decision tree model, which is generally insensitive to the scale of input features.\n",
    "\n",
    "C. When dealing with time series data that exhibits a normal distribution, to maintain the properties of the data.\n",
    "\n",
    "D. When applying a gradient descent optimization algorithm to a dataset with features that have vastly different scales, such as height in centimeters and weight in kilograms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q11-Decision Tree for Movie Preferences (20 Points)\n",
    "\n",
    "You are working on a project to predict whether a person prefers a particular genre of movies based on their characteristics. Consider a dataset of moviegoers with the following features:\n",
    "\n",
    "- Genre: {Action, Drama}\n",
    "- Age Group: {Teen, Adult}\n",
    "- Ticket Price: {Regular, VIP}\n",
    "\n",
    "Here is the dataset:\n",
    "\n",
    "| Genre  | Age Group | Ticket Price | Preference? |\n",
    "|--------|-----------|--------------|-------------|\n",
    "| Action | Adult     | VIP          | Yes         |\n",
    "| Drama  | Adult     | Regular      | Yes         |\n",
    "| Drama  | Adult     | Regular      | Yes         |\n",
    "| Action | Adult     | VIP          | No          |\n",
    "| Action | Teen      | Regular      | No          |\n",
    "| Action | Adult     | Regular      | Yes         |\n",
    "| Drama  | Teen      | Regular      | Yes         |\n",
    "| Drama  | Teen      | VIP          | Yes         |\n",
    "| Action | Teen      | Regular      | Yes         |\n",
    "| Drama  | Adult     | Regular      | No          |\n",
    "| Drama  | Adult     | VIP          | Yes         |\n",
    "| Drama  | Teen      | Regular      | Yes         |\n",
    "| Drama  | Adult     | VIP          | No          |\n",
    "| Drama  | Teen      | VIP          | Yes         |\n",
    "| Action | Teen      | VIP          | No          |\n",
    "| Action | Teen      | VIP          | No          |\n",
    "\n",
    "Note that samples with the same features can have different labels. If the leaf node of a decision tree is not pure, the majority vote is used to determine the output label.\n",
    "\n",
    "Now, your goal is to build a decision tree to predict whether a person prefers a particular genre of movies. In particular, you want to know which feature (among Genre, Age Group, and Ticket Price) is the most important feature to predict the preference. In other words, which feature should be the root node of the decision tree to maximize the information gain?\n",
    "\n",
    "Your tasks:\n",
    "\n",
    "1. Compute Gini impurity at the root node. (5 points)\n",
    "2. For the 3 features (Genre, Age Group, and Ticket Price), compute the information gain if that feature is used to split the root node. (10 points)\n",
    "3. Conclude which feature is the most important feature to predict the preference as it maximizes the information gain. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Gini Impurity at the Root Notde**\n",
    "\n",
    "The Gini Impurity is calculated as:\n",
    "$$ I_G(t)=1-\\sum_{i=1}^{k} p_i^2 $$\n",
    "Where $p_i$ is the proportion of samples belonging to class $i$. For the entire dateset:\n",
    "- Yes preference: 9\n",
    "- No preference: 7\n",
    "- Total: 16\n",
    "\n",
    "$$ I_G(t)=1-((9/16)^2+(7/16)^2)=0.4922 $$\n",
    "\n",
    "**2. Information Gain for Each Feature**\n",
    "\n",
    "**For Genre:**\n",
    "\n",
    "- Action: 7 (4 Yes, 3 No)\n",
    "- Drama: 9 (5 Yes, 4 No)\n",
    "\n",
    "$$ I_G(Action) = 1-((4/7)^2+(3/7)^2) = 0.4898 $$\n",
    "$$ I_G(Drama) = 1-((5/9)^2+(4/9)^2) = 0.4938 $$\n",
    "$$ IG_{Genre} = I_G(t)-7/16*I_G(Action)-9/16*I_G(Drama)=0.0005 $$\n",
    "\n",
    "**For Age Group:**\n",
    "\n",
    "- Teen: 8 (5 Yes, 3 No)\n",
    "- Adult: 8 (4 Yes, 4 No)\n",
    "\n",
    "$$ I_G(Teen) = 1-((5/8)^2+(3/8)^2) = 0.4688 $$\n",
    "$$ I_G(Adult) = 1-((4/8)^2+(4/8)^2) = 0.5 $$\n",
    "$$ IG_{Age} = I_G(t)-8/16*I_G(Action)-8/16*I_G(Drama)=0.0078 $$\n",
    "\n",
    "**For Ticket Price:**\n",
    "\n",
    "- Regular: 9 (6 Yes, 3 No)\n",
    "- VIP: 7 (3 Yes, 4 No)\n",
    "\n",
    "$$ I_G(Regular) = 1-((6/9)^2+(3/9)^2) = 0.4444 $$\n",
    "$$ I_G(VIP) = 1-((3/7)^2+(4/7)^2) = 0.4898 $$\n",
    "$$ IG_{Price} = I_G(t)-9/16*I_G(Action)-7/16*I_G(Drama)=0.0268 $$\n",
    "\n",
    "**3. Conclusion**\n",
    "\n",
    "The feature that maximizes the information gain is **Ticket Price**, with an information gain of **0.0268**."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
