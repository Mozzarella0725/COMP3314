{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AUwbFEaMiWi"
   },
   "source": [
    "# COMP3314 - Assignment 2 Question 2: Spam classifier with MLP (30 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T18:21:06.553305Z",
     "start_time": "2022-10-26T18:21:06.548561Z"
    },
    "id": "ItQpEzhRAomC"
   },
   "source": [
    "## Step 1: Download and pre-process (code given)\n",
    "This step has been provided to you. Do not modify the code in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:05:59.155299Z",
     "start_time": "2022-10-27T05:05:58.015019Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vbs5g3YbuH59",
    "outputId": "a6288dcd-5dde-44c7-ab3a-b3fea4887280"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Do not change the code inside this cell.\n",
    "\"\"\"\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import tarfile\n",
    "from urllib.request import urlretrieve\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "class EmailCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        no_header=True,\n",
    "        to_lowercase=True,\n",
    "        url_to_word=True,\n",
    "        num_to_word=True,\n",
    "        remove_punc=True,\n",
    "    ):\n",
    "        self.no_header = no_header\n",
    "        self.to_lowercase = to_lowercase\n",
    "        self.url_to_word = url_to_word\n",
    "        self.num_to_word = num_to_word\n",
    "        self.remove_punc = remove_punc\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        cleaned_emails = []\n",
    "        for email in X:\n",
    "            if self.no_header:\n",
    "                email = self.remove_header(email)\n",
    "            if self.to_lowercase:\n",
    "                email = email.lower()\n",
    "\n",
    "            words = email.split()\n",
    "            if self.url_to_word:\n",
    "                words = self.convert_url_to_word(words)\n",
    "            if self.num_to_word:\n",
    "                words = self.convert_num_to_word(words)\n",
    "            email = \" \".join(words)\n",
    "            if self.remove_punc:\n",
    "                email = \"\".join([c for c in email if c.isalnum() or c.isspace()])\n",
    "            cleaned_emails.append(email)\n",
    "        return cleaned_emails\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_header(email):\n",
    "        return email[email.index(\"\\n\\n\") :]\n",
    "\n",
    "    @staticmethod\n",
    "    def is_url(string):\n",
    "        return re.match(\n",
    "            \"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n",
    "            string,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_url_to_word(words):\n",
    "        return [\"URL\" if EmailCleaner.is_url(word) else word for word in words]\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_num_to_word(words):\n",
    "        return [\"NUM\" if word.isdigit() else word for word in words]\n",
    "\n",
    "\n",
    "def download_and_extract(url, dataset_dir=\"data\"):\n",
    "    tar_dir = os.path.join(dataset_dir, \"tar\")\n",
    "    os.makedirs(tar_dir, exist_ok=True)\n",
    "    filename = url.rsplit(\"/\", 1)[-1]\n",
    "    tarpath = os.path.join(tar_dir, filename)\n",
    "\n",
    "    class DownloadProgressBar(tqdm):\n",
    "        def update_to(self, b=1, bsize=1, tsize=None):\n",
    "            if tsize is not None:\n",
    "                self.total = tsize\n",
    "            self.update(b * bsize - self.n)\n",
    "\n",
    "    if not os.path.exists(tarpath):\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        with DownloadProgressBar(\n",
    "            unit=\"B\", unit_scale=True, miniters=1, desc=url.split(\"/\")[-1]\n",
    "        ) as t:\n",
    "            urlretrieve(url, tarpath, reporthook=t.update_to)\n",
    "        print(\"\\nDownload completed.\")\n",
    "    else:\n",
    "        print(f\"{filename} already downloaded.\")\n",
    "\n",
    "    print(\"Extracting files...\")\n",
    "    with tarfile.open(tarpath) as tar:\n",
    "        dirname = os.path.join(dataset_dir, tar.getmembers()[0].name.split(\"/\")[0])\n",
    "        if os.path.isdir(dirname):\n",
    "            shutil.rmtree(dirname)\n",
    "        tar.extractall(path=dataset_dir)\n",
    "    print(\"Extraction completed.\")\n",
    "\n",
    "    cmds_path = os.path.join(dirname, \"cmds\")\n",
    "    if os.path.isfile(cmds_path):\n",
    "        os.remove(cmds_path)\n",
    "    return dirname\n",
    "\n",
    "\n",
    "def load_dataset(dirpath):\n",
    "    files = []\n",
    "    filepaths = glob.glob(os.path.join(dirpath, \"*\"))\n",
    "    for path in filepaths:\n",
    "        with open(path, \"rb\") as f:\n",
    "            content = f.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "            files.append(content)\n",
    "    return files\n",
    "\n",
    "\n",
    "def download_datasets():\n",
    "    spam_url = \"https://github.com/comp3314/hw-data/releases/download/hw3/20050311_spam_2.tar.bz2\"\n",
    "    easy_ham_url = \"https://github.com/comp3314/hw-data/releases/download/hw3/20030228_easy_ham_2.tar.bz2\"\n",
    "    hard_ham_url = \"https://github.com/comp3314/hw-data/releases/download/hw3/20030228_hard_ham.tar.bz2\"\n",
    "\n",
    "    spam = load_dataset(download_and_extract(spam_url))\n",
    "    easy_ham = load_dataset(download_and_extract(easy_ham_url))\n",
    "    hard_ham = load_dataset(download_and_extract(hard_ham_url))\n",
    "\n",
    "    X = spam + easy_ham + hard_ham\n",
    "    y = np.concatenate((np.ones(len(spam)), np.zeros(len(easy_ham) + len(hard_ham))))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Do not change the code inside this cell.\n",
    "\"\"\"\n",
    "\n",
    "# Download and prepare the dataset\n",
    "print(\"Starting dataset download and preparation...\")\n",
    "X, y = download_datasets()\n",
    "print(\"Dataset preparation completed.\")\n",
    "\n",
    "# Shuffle and split the dataset\n",
    "X, y = shuffle(X, y, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "print(f\"The number of training samples: {len(X_train)}\")\n",
    "print(f\"The number of test samples: {len(X_test)}\")\n",
    "\n",
    "# Preprocess the data\n",
    "print(\"Starting preprocessing...\")\n",
    "email_cleaner = EmailCleaner()\n",
    "count_vectorizer = CountVectorizer()\n",
    "prepare_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"email_cleaner\", email_cleaner),\n",
    "        (\"count_vectorizer\", count_vectorizer),\n",
    "    ]\n",
    ")\n",
    "X_all = X_train + X_test\n",
    "prepare_pipeline.fit(X_all)\n",
    "X_all_transformed = prepare_pipeline.transform(X_all)\n",
    "num_train = len(X_train)\n",
    "X_train = X_all_transformed[:num_train]\n",
    "X_test = X_all_transformed[num_train:]\n",
    "print(\"Preprocessing completed.\")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZYuW1qnvth8"
   },
   "source": [
    "## Step 2: Train spam classifiers with MLP (10 points)\n",
    "\n",
    "Next, let's build a spam classifier with `MLPClassifier` of sklearn.\n",
    "\n",
    "First, implement the following MLP configurations:\n",
    "1. 1 hidden layer with 10 neurons\n",
    "2. 1 hidden layer with 20 neurons\n",
    "3. 1 hidden layer with 40 neurons\n",
    "4. 2 hidden layers with 5 neuron in each hidden layer\n",
    "5. 2 hidden layers with 10 neurons in each hidden layer\n",
    "6. 2 hidden layers with 20 neurons in each hidden layer\n",
    "\n",
    "Then, train your nerual networks by calling the `.fit()` function on the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:06:25.443785Z",
     "start_time": "2022-10-27T05:06:25.420443Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yRP2qwQAAomF",
    "outputId": "916d66dc-a2a7-4b6a-9c71-1abc1882e3c5"
   },
   "outputs": [],
   "source": [
    "# === Your code here ===\n",
    "# ======================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W17SOnErAomF"
   },
   "source": [
    "## Step 3: Evaluate your classifiers (10 points)\n",
    "\n",
    "Evaluate your classifier with the test set and report the precision, recall, and accuracy for each setting of the hyper-parameters. What conclusion could you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:06:26.371331Z",
     "start_time": "2022-10-27T05:06:26.364811Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DrXaSqBIAomF",
    "outputId": "19fd9a6b-156a-4bb1-bc0c-82cb05855199"
   },
   "outputs": [],
   "source": [
    "# === Your code here ===\n",
    "# ======================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTbxtjvHohqJ"
   },
   "source": [
    "## Step 4: Ensemble of classifiers (10 points)\n",
    "\n",
    "Now, pick 3 of the classifiers you have trained in the previous step and ensemble them. Report the precision, recall, and accuracy of your ensemble classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:06:29.164535Z",
     "start_time": "2022-10-27T05:06:28.530051Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QwS_cYldop6_",
    "outputId": "b88ccd6a-10c9-40f6-9486-3e62bc1ebfd4"
   },
   "outputs": [],
   "source": [
    "# === Your code here ===\n",
    "\n",
    "# ======================"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "49c48b447b9efd9c07d32c0dec9df5e0c02b4225e51c3003920687e790460610"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
