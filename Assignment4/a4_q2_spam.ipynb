{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AUwbFEaMiWi"
   },
   "source": [
    "# COMP3314 - Assignment 2 Question 2: Spam classifier with MLP (30 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T18:21:06.553305Z",
     "start_time": "2022-10-26T18:21:06.548561Z"
    },
    "id": "ItQpEzhRAomC"
   },
   "source": [
    "## Step 1: Download and pre-process (code given)\n",
    "This step has been provided to you. Do not modify the code in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:05:59.155299Z",
     "start_time": "2022-10-27T05:05:58.015019Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vbs5g3YbuH59",
    "outputId": "a6288dcd-5dde-44c7-ab3a-b3fea4887280"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Do not change the code inside this cell.\n",
    "\"\"\"\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import tarfile\n",
    "from urllib.request import urlretrieve\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "class EmailCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        no_header=True,\n",
    "        to_lowercase=True,\n",
    "        url_to_word=True,\n",
    "        num_to_word=True,\n",
    "        remove_punc=True,\n",
    "    ):\n",
    "        self.no_header = no_header\n",
    "        self.to_lowercase = to_lowercase\n",
    "        self.url_to_word = url_to_word\n",
    "        self.num_to_word = num_to_word\n",
    "        self.remove_punc = remove_punc\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        cleaned_emails = []\n",
    "        for email in X:\n",
    "            if self.no_header:\n",
    "                email = self.remove_header(email)\n",
    "            if self.to_lowercase:\n",
    "                email = email.lower()\n",
    "\n",
    "            words = email.split()\n",
    "            if self.url_to_word:\n",
    "                words = self.convert_url_to_word(words)\n",
    "            if self.num_to_word:\n",
    "                words = self.convert_num_to_word(words)\n",
    "            email = \" \".join(words)\n",
    "            if self.remove_punc:\n",
    "                email = \"\".join([c for c in email if c.isalnum() or c.isspace()])\n",
    "            cleaned_emails.append(email)\n",
    "        return cleaned_emails\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_header(email):\n",
    "        return email[email.index(\"\\n\\n\") :]\n",
    "\n",
    "    @staticmethod\n",
    "    def is_url(string):\n",
    "        return re.match(\n",
    "            \"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n",
    "            string,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_url_to_word(words):\n",
    "        return [\"URL\" if EmailCleaner.is_url(word) else word for word in words]\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_num_to_word(words):\n",
    "        return [\"NUM\" if word.isdigit() else word for word in words]\n",
    "\n",
    "\n",
    "def download_and_extract(url, dataset_dir=\"data\"):\n",
    "    tar_dir = os.path.join(dataset_dir, \"tar\")\n",
    "    os.makedirs(tar_dir, exist_ok=True)\n",
    "    filename = url.rsplit(\"/\", 1)[-1]\n",
    "    tarpath = os.path.join(tar_dir, filename)\n",
    "\n",
    "    class DownloadProgressBar(tqdm):\n",
    "        def update_to(self, b=1, bsize=1, tsize=None):\n",
    "            if tsize is not None:\n",
    "                self.total = tsize\n",
    "            self.update(b * bsize - self.n)\n",
    "\n",
    "    if not os.path.exists(tarpath):\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        with DownloadProgressBar(\n",
    "            unit=\"B\", unit_scale=True, miniters=1, desc=url.split(\"/\")[-1]\n",
    "        ) as t:\n",
    "            urlretrieve(url, tarpath, reporthook=t.update_to)\n",
    "        print(\"\\nDownload completed.\")\n",
    "    else:\n",
    "        print(f\"{filename} already downloaded.\")\n",
    "\n",
    "    print(\"Extracting files...\")\n",
    "    with tarfile.open(tarpath) as tar:\n",
    "        dirname = os.path.join(dataset_dir, tar.getmembers()[0].name.split(\"/\")[0])\n",
    "        if os.path.isdir(dirname):\n",
    "            shutil.rmtree(dirname)\n",
    "        tar.extractall(path=dataset_dir)\n",
    "    print(\"Extraction completed.\")\n",
    "\n",
    "    cmds_path = os.path.join(dirname, \"cmds\")\n",
    "    if os.path.isfile(cmds_path):\n",
    "        os.remove(cmds_path)\n",
    "    return dirname\n",
    "\n",
    "\n",
    "def load_dataset(dirpath):\n",
    "    files = []\n",
    "    filepaths = glob.glob(os.path.join(dirpath, \"*\"))\n",
    "    for path in filepaths:\n",
    "        with open(path, \"rb\") as f:\n",
    "            content = f.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "            files.append(content)\n",
    "    return files\n",
    "\n",
    "\n",
    "def download_datasets():\n",
    "    spam_url = \"https://github.com/comp3314/hw-data/releases/download/hw3/20050311_spam_2.tar.bz2\"\n",
    "    easy_ham_url = \"https://github.com/comp3314/hw-data/releases/download/hw3/20030228_easy_ham_2.tar.bz2\"\n",
    "    hard_ham_url = \"https://github.com/comp3314/hw-data/releases/download/hw3/20030228_hard_ham.tar.bz2\"\n",
    "\n",
    "    spam = load_dataset(download_and_extract(spam_url))\n",
    "    easy_ham = load_dataset(download_and_extract(easy_ham_url))\n",
    "    hard_ham = load_dataset(download_and_extract(hard_ham_url))\n",
    "\n",
    "    X = spam + easy_ham + hard_ham\n",
    "    y = np.concatenate((np.ones(len(spam)), np.zeros(len(easy_ham) + len(hard_ham))))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset download and preparation...\n",
      "Downloading 20050311_spam_2.tar.bz2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20050311_spam_2.tar.bz2: 2.06MB [00:03, 559kB/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download completed.\n",
      "Extracting files...\n",
      "Extraction completed.\n",
      "Downloading 20030228_easy_ham_2.tar.bz2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20030228_easy_ham_2.tar.bz2: 1.08MB [00:03, 356kB/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download completed.\n",
      "Extracting files...\n",
      "Extraction completed.\n",
      "Downloading 20030228_hard_ham.tar.bz2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20030228_hard_ham.tar.bz2: 1.03MB [00:02, 379kB/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download completed.\n",
      "Extracting files...\n",
      "Extraction completed.\n",
      "Dataset preparation completed.\n",
      "The number of training samples: 2436\n",
      "The number of test samples: 610\n",
      "Starting preprocessing...\n",
      "Preprocessing completed.\n",
      "(2436, 108735)\n",
      "(610, 108735)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Do not change the code inside this cell.\n",
    "\"\"\"\n",
    "\n",
    "# Download and prepare the dataset\n",
    "print(\"Starting dataset download and preparation...\")\n",
    "X, y = download_datasets()\n",
    "print(\"Dataset preparation completed.\")\n",
    "\n",
    "# Shuffle and split the dataset\n",
    "X, y = shuffle(X, y, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "print(f\"The number of training samples: {len(X_train)}\")\n",
    "print(f\"The number of test samples: {len(X_test)}\")\n",
    "\n",
    "# Preprocess the data\n",
    "print(\"Starting preprocessing...\")\n",
    "email_cleaner = EmailCleaner()\n",
    "count_vectorizer = CountVectorizer()\n",
    "prepare_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"email_cleaner\", email_cleaner),\n",
    "        (\"count_vectorizer\", count_vectorizer),\n",
    "    ]\n",
    ")\n",
    "X_all = X_train + X_test\n",
    "prepare_pipeline.fit(X_all)\n",
    "X_all_transformed = prepare_pipeline.transform(X_all)\n",
    "num_train = len(X_train)\n",
    "X_train = X_all_transformed[:num_train]\n",
    "X_test = X_all_transformed[num_train:]\n",
    "print(\"Preprocessing completed.\")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZYuW1qnvth8"
   },
   "source": [
    "## Step 2: Train spam classifiers with MLP (10 points)\n",
    "\n",
    "Next, let's build a spam classifier with `MLPClassifier` of sklearn.\n",
    "\n",
    "First, implement the following MLP configurations:\n",
    "1. 1 hidden layer with 10 neurons\n",
    "2. 1 hidden layer with 20 neurons\n",
    "3. 1 hidden layer with 40 neurons\n",
    "4. 2 hidden layers with 5 neuron in each hidden layer\n",
    "5. 2 hidden layers with 10 neurons in each hidden layer\n",
    "6. 2 hidden layers with 20 neurons in each hidden layer\n",
    "\n",
    "Then, train your nerual networks by calling the `.fit()` function on the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:06:25.443785Z",
     "start_time": "2022-10-27T05:06:25.420443Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yRP2qwQAAomF",
    "outputId": "916d66dc-a2a7-4b6a-9c71-1abc1882e3c5"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# List of MLP configurations\n",
    "mlp_configs = [\n",
    "    (10,),  # 1 hidden layer with 10 neurons\n",
    "    (20,),  # 1 hidden layer with 20 neurons\n",
    "    (40,),  # 1 hidden layer with 40 neurons\n",
    "    (5, 5),  # 2 hidden layers with 5 neurons each\n",
    "    (10, 10),  # 2 hidden layers with 10 neurons each\n",
    "    (20, 20),  # 2 hidden layers with 20 neurons each\n",
    "]\n",
    "\n",
    "# Dictionary to store the trained models\n",
    "trained_models = {}\n",
    "\n",
    "# Train the classifiers with different configurations and store them\n",
    "for config in mlp_configs:\n",
    "    # Initialize MLPClassifier with the current configuration\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=config, max_iter=1000, random_state=42)\n",
    "    \n",
    "    # Train the model\n",
    "    mlp.fit(X_train, y_train)\n",
    "    \n",
    "    # Store the trained model in the dictionary with the configuration as the key\n",
    "    trained_models[config] = mlp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W17SOnErAomF"
   },
   "source": [
    "## Step 3: Evaluate your classifiers (10 points)\n",
    "\n",
    "Evaluate your classifier with the test set and report the precision, recall, and accuracy for each setting of the hyper-parameters. What conclusion could you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:06:26.371331Z",
     "start_time": "2022-10-27T05:06:26.364811Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DrXaSqBIAomF",
    "outputId": "19fd9a6b-156a-4bb1-bc0c-82cb05855199"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration (10,):\n",
      "  Accuracy: 0.9852\n",
      "  Precision: 0.9855\n",
      "  Recall: 0.9849\n",
      "  F1-score: 0.9852\n",
      "\n",
      "Configuration (20,):\n",
      "  Accuracy: 0.9869\n",
      "  Precision: 0.9868\n",
      "  Recall: 0.9868\n",
      "  F1-score: 0.9868\n",
      "\n",
      "Configuration (40,):\n",
      "  Accuracy: 0.9902\n",
      "  Precision: 0.9906\n",
      "  Recall: 0.9897\n",
      "  F1-score: 0.9901\n",
      "\n",
      "Configuration (5, 5):\n",
      "  Accuracy: 0.9754\n",
      "  Precision: 0.9779\n",
      "  Recall: 0.9738\n",
      "  F1-score: 0.9752\n",
      "\n",
      "Configuration (10, 10):\n",
      "  Accuracy: 0.9852\n",
      "  Precision: 0.9861\n",
      "  Recall: 0.9845\n",
      "  F1-score: 0.9852\n",
      "\n",
      "Configuration (20, 20):\n",
      "  Accuracy: 0.9869\n",
      "  Precision: 0.9870\n",
      "  Recall: 0.9866\n",
      "  F1-score: 0.9868\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === Your code here ===\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Dictionary to store the evaluation results\n",
    "evaluation_results = {}\n",
    "\n",
    "# Evaluate the classifiers on the test set\n",
    "for config, mlp in trained_models.items():\n",
    "    # Make predictions on the test set\n",
    "    y_pred = mlp.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Generate classification report (precision, recall, f1-score)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Store the results in the dictionary\n",
    "    evaluation_results[config] = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": report['macro avg']['precision'],\n",
    "        \"recall\": report['macro avg']['recall'],\n",
    "        \"f1-score\": report['macro avg']['f1-score']\n",
    "    }\n",
    "\n",
    "    # Optionally, print the results for each configuration\n",
    "    print(f\"Configuration {config}:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {report['macro avg']['precision']:.4f}\")\n",
    "    print(f\"  Recall: {report['macro avg']['recall']:.4f}\")\n",
    "    print(f\"  F1-score: {report['macro avg']['f1-score']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# ======================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "- The models with one hidden layer (with 10, 20, or 40 neurons) generally show a higher accuracy compared to models with two hidden layers.\n",
    "- Configuration (40,) performs the best in terms of accuracy (0.9902), precision, recall, and F1-score.\n",
    "- The two-layer configurations like (5, 5), (10, 10) show a slight drop in accuracy compared to the one-layer configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTbxtjvHohqJ"
   },
   "source": [
    "## Step 4: Ensemble of classifiers (10 points)\n",
    "\n",
    "Now, pick 3 of the classifiers you have trained in the previous step and ensemble them. Report the precision, recall, and accuracy of your ensemble classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-27T05:06:29.164535Z",
     "start_time": "2022-10-27T05:06:28.530051Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QwS_cYldop6_",
    "outputId": "b88ccd6a-10c9-40f6-9486-3e62bc1ebfd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Classifier (Hard Voting):\n",
      "  Accuracy: 0.9885\n",
      "  Precision: 0.9886\n",
      "  Recall: 0.9884\n",
      "  F1-score: 0.9885\n"
     ]
    }
   ],
   "source": [
    "# === Your code here ===\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Select three classifiers from the trained models (e.g., with 10, 20, and 40 neurons)\n",
    "chosen_configs = [(10,), (20,), (40,)]\n",
    "models_to_ensemble = [trained_models[config] for config in chosen_configs]\n",
    "\n",
    "# Create a VotingClassifier (hard voting)\n",
    "voting_clf = VotingClassifier(estimators=[(f'mlp_{config}', model) for config, model in zip(chosen_configs, models_to_ensemble)], voting='hard')\n",
    "\n",
    "# Train the ensemble model\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Ensemble Classifier (Hard Voting):\")\n",
    "print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "print(f\"  Precision: {report['macro avg']['precision']:.4f}\")\n",
    "print(f\"  Recall: {report['macro avg']['recall']:.4f}\")\n",
    "print(f\"  F1-score: {report['macro avg']['f1-score']:.4f}\")\n",
    "\n",
    "# ======================"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "49c48b447b9efd9c07d32c0dec9df5e0c02b4225e51c3003920687e790460610"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
